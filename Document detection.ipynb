{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41a3508c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fc3ecad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image  module rotate or convert tif in jpg\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import PIL \n",
    "from PIL import Image \n",
    "\n",
    "\n",
    "\n",
    "class image_converter():\n",
    "    def __init__(self, abs_path = ''):\n",
    "        if abs_path:\n",
    "            self.main_path = abs_path\n",
    "        else:\n",
    "            self.main_path = os.path.join(os.getcwd(), 'data')\n",
    "        \n",
    "        self.dir_json = list_dir_fls(folder=self.main_path)\n",
    "    \n",
    "    \n",
    "        \n",
    "    def list_dir_fls(self, folder='data', current_folder = True):\n",
    "        \"\"\"return path dictionary\"\"\"\n",
    "        answer = {'folder': '', 'subfolders': [], 'files': []}\n",
    "        try:\n",
    "            if current_folder:\n",
    "                data_path = os.path.join(prog_path, folder)\n",
    "            else:\n",
    "                data_path = folder\n",
    "            answer['folder'] = data_path\n",
    "            folders = [os.path.join(data_path, name) for name in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, name))]\n",
    "            files = [os.path.join(data_path, name) for name in os.listdir(data_path) if os.path.isfile(os.path.join(data_path, name))]\n",
    "            for fold in folders:\n",
    "                answer['subfolders'].append(list_dir_fls(folder=fold, current_folder = False))\n",
    "            answer['files'] = files\n",
    "        except Exception as e:\n",
    "            print('Error in module list_dir', e)\n",
    "\n",
    "#         with open('folders_tree.json', 'w') as outfile:\n",
    "#             json.dump(answer, outfile)\n",
    "        return(answer)\n",
    "\n",
    "    def rotate_files(self):\n",
    "        \"\"\"rotate file 3 times clockwise 90 degree and write as _90 _180 _270\"\"\"\n",
    "        files_list = self.dir_json['files']\n",
    "        print(files_list)\n",
    "        for file in files_list:\n",
    "            splited_tag = file.split('/')\n",
    "            file_folder = '/'.join(splited_tag[0:-1])\n",
    "            file_name = splited_tag[-1]\n",
    "            file_abs_name = file_name.split('.')[0:-1]\n",
    "            file_abs_name = '.'.join(file_abs_name)\n",
    "            file_extention = file_name.split('.')[-1]\n",
    "            if file_extention == 'jpg':\n",
    "                for degree in [90, 180,270]:\n",
    "                    pil_image = Image.open(file)\n",
    "                    rotated = pil_image.rotate(degree, expand=True)\n",
    "                    file_name_new = f'{file_abs_name}_{degree}.{file_extention}'\n",
    "                    file_save = rotated.save(f'{file_folder}/{file_name_new}')\n",
    "                    print(file_name_new)\n",
    "        pass\n",
    "\n",
    "    def rename_tif_files(self):\n",
    "        \"\"\" rename and convert tif ro jpg in directory\"\"\"\n",
    "        list_folders_files = self.dir_json['files']\n",
    "        main_folder_path = self.dir_json['folder']\n",
    "        self.make_jpg_dir()\n",
    "        for sub_f in self.dir_json['subfolders']:\n",
    "            num = 0\n",
    "            folder_path = sub_f['folder']\n",
    "            folder_name = folder_path.split('/')[-1]\n",
    "            for file_path in sub_f['files']:\n",
    "                abs_name = file_path.split('/')[-1]\n",
    "                extention = str(abs_name).split('.')[-1]\n",
    "                if extention == 'tif':\n",
    "                    new_abs_name = f'{folder_name}.{num}.jpg'\n",
    "                    new_name = f'{self.main_path}/jpg/{new_abs_name}'\n",
    "                    read = cv2.imread(file_path)\n",
    "                    cv2.imwrite(new_name,read,[int(cv2.IMWRITE_JPEG_QUALITY), 100])   \n",
    "                    # os.rename(file_path, new_name) #it just rename -not convert\n",
    "                    num+=1\n",
    "                else:\n",
    "                    continue\n",
    "    \n",
    "    def make_jpg_dir(self):\n",
    "        try:\n",
    "            os.mkdir(f'{self.main_path}/jpg')\n",
    "        except Exception as e:\n",
    "            print(f'folder {self.main_path} already exist', e)\n",
    "                \n",
    "# def del_file(folder_path='data/marked', ext='jpg'):\n",
    "#   \"\"\"clear files with extentions \"\"\"\n",
    "#   list_folders_files = list_dir_fls(folder=folder_path)\n",
    "#   for sub_f in list_folders_files['subfolders']:\n",
    "#     num = 0\n",
    "#     folder_path = sub_f['folder']\n",
    "#     folder_abs_name = sub_f['folder'].split('/')[-1]\n",
    "#     for file_path in sub_f['files']:\n",
    "#       abs_name = file_path.split('/')[-1]\n",
    "#       extention = abs_name.split('.')\n",
    "#       if extention == ext:\n",
    "#         os.remove(file_path)\n",
    "\n",
    "#convertion = image_converter('/Users/johnlennon/My Drive/Python/Pdf/data/label_annotated/upd')\n",
    "#convertion = image_converter('/Users/johnlennon/My Drive/Python/Pdf/data/label_annotated/invoice')\n",
    "#convertion.rename_tif_files()\n",
    "#convertion.rotate_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eb3d7d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/johnlennon/My Drive/Python/Pdf/data/label_annotated_train/test/test.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/25/6_wvkwfn6bn4n2j_2fmk5v740000gp/T/ipykernel_4760/3451162352.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mxml_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxml_to_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/johnlennon/My Drive/Python/Pdf/data/label_annotated_train/test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/25/6_wvkwfn6bn4n2j_2fmk5v740000gp/T/ipykernel_4760/3451162352.py\u001b[0m in \u001b[0;36mxml_to_csv\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mxml_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxml_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mdir_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mxml_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{path}/{dir_name}.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Successfully converted .xml to .csv in directory {dir_name}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mxml_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf_venv/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3464\u001b[0m         )\n\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3466\u001b[0;31m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[1;32m   3467\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3468\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf_venv/lib/python3.9/site-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1103\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         )\n\u001b[0;32m-> 1105\u001b[0;31m         \u001b[0mcsv_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf_venv/lib/python3.9/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \"\"\"\n\u001b[1;32m    236\u001b[0m         \u001b[0;31m# apply compression and byte/text conversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         with get_handle(\n\u001b[0m\u001b[1;32m    238\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf_venv/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/johnlennon/My Drive/Python/Pdf/data/label_annotated_train/test/test.csv'"
     ]
    }
   ],
   "source": [
    "# module for convert xml to csv after labelim annotations\n",
    "import xml.etree.ElementTree as ET\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "def xml_to_csv(path):\n",
    "    \"\"\"Iterates through all .xml files (generated by labelImg) in a given directory and combines\n",
    "    them in a single Pandas dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    path : str\n",
    "        The path containing the .xml files\n",
    "    Returns\n",
    "    -------\n",
    "    Pandas DataFrame\n",
    "        The produced dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    xml_list = []\n",
    "    for xml_file in glob.glob(path + '/*.xml'):\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        for member in root.findall('object'):\n",
    "            value = (root.find('filename').text,\n",
    "                     int(root.find('size')[0].text),\n",
    "                     int(root.find('size')[1].text),\n",
    "                     member[0].text,\n",
    "                     int(member[4][0].text),\n",
    "                     int(member[4][1].text),\n",
    "                     int(member[4][2].text),\n",
    "                     int(member[4][3].text)\n",
    "                     )\n",
    "            xml_list.append(value)\n",
    "    column_name = ['filename', 'width', 'height',\n",
    "                   'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
    "    xml_df = pd.DataFrame(xml_list, columns=column_name)\n",
    "    dir_name = path.split('/')[-1]\n",
    "    xml_df.to_csv(f'{path}/{dir_name}.csv', sep='\\t', encoding='utf-8', index=False)\n",
    "    print(f'Successfully converted .xml to .csv in directory {dir_name}')\n",
    "    return xml_df\n",
    "\n",
    "print(xml_to_csv('/Users/johnlennon/My Drive/Python/Pdf/data/label_annotated_train/test'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89a29d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create tensorflow records\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_tf_example(group, path):\n",
    "    with tf.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n",
    "        encoded_jpg = fid.read()\n",
    "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
    "    image = Image.open(encoded_jpg_io)\n",
    "    width, height = image.size\n",
    "\n",
    "    filename = group.filename.encode('utf8')\n",
    "    image_format = b'jpg'\n",
    "    xmins = []\n",
    "    xmaxs = []\n",
    "    ymins = []\n",
    "    ymaxs = []\n",
    "    classes_text = []\n",
    "    classes = []\n",
    "\n",
    "    for index, row in group.object.iterrows():\n",
    "        xmins.append(row['xmin'] / width)\n",
    "        xmaxs.append(row['xmax'] / width)\n",
    "        ymins.append(row['ymin'] / height)\n",
    "        ymaxs.append(row['ymax'] / height)\n",
    "        classes_text.append(row['class'].encode('utf8'))\n",
    "        classes.append(class_text_to_int(row['class']))\n",
    "\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image/height': dataset_util.int64_feature(height),\n",
    "        'image/width': dataset_util.int64_feature(width),\n",
    "        'image/filename': dataset_util.bytes_feature(filename),\n",
    "        'image/source_id': dataset_util.bytes_feature(filename),\n",
    "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
    "        'image/format': dataset_util.bytes_feature(image_format),\n",
    "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
    "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
    "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
    "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
    "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
    "    }))\n",
    "    return tf_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eb4dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'sf_serman.pdf'\n",
    "file_name.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921a2000",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [1,2,3]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb00272b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
